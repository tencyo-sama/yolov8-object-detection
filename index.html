<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>YOLOv8 Object Detection</title>
  <!-- TensorFlow.js の読み込み -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
  <style>
    /* ... (スタイルは変更なし) ... */
    body {
      margin: 0;
      padding: 0;
      width: 100vw;
      height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      background: #000;
    }
    .container {
      width: 100%;
      height: 85vh;
      position: relative;
      overflow: hidden;
    }
    #output {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    .button-container {
      position: fixed;
      bottom: 20px;
      width: 100%;
      display: flex;
      justify-content: center;
      gap: 20px;
      z-index: 1000;
    }
    button {
      padding: 15px 30px;
      font-size: 18px;
      border-radius: 25px;
      border: none;
      background: rgba(255, 255, 255, 0.9);
      box-shadow: 0 2px 5px rgba(0,0,0,0.2);
      cursor: pointer;
    }
    button:disabled {
      background: rgba(200, 200, 200, 0.9);
      cursor: not-allowed;
    }
    .message {
      position: fixed;
      bottom: 100px;
      left: 50%;
      transform: translateX(-50%);
      background-color: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 10px 20px;
      border-radius: 5px;
      font-size: 16px;
      z-index: 1000;
      text-align: center;
      max-width: 80%;
    }
    .loading-indicator {
      border: 4px solid #f3f3f3;
      border-radius: 50%;
      border-top: 4px solid #3498db;
      width: 20px;
      height: 20px;
      animation: spin 1s linear infinite;
      display: none;
      margin: 10px auto;
    }
    @keyframes spin {
      0% { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }
  </style>
</head>
<body>
  <div class="container">
    <canvas id="output"></canvas>
    <video id="input" style="display: none;" playsinline autoplay muted></video>
  </div>
  <div class="message" id="message">
    <div class="loading-indicator" id="loading"></div>
    <div id="message-text"></div>
  </div>
  <div class="button-container">
    <button id="start">開始</button>
    <button id="stop" disabled>停止</button>
  </div>

  <script>
    let model;
    let isRunning = false;
    let video, canvas, messageDiv, messageText, loadingIndicator, startButton, stopButton, ctx;
    let animationFrameId;

    //モデルの入力サイズ
    const MODEL_INPUT_WIDTH = 640;  // YOLOv8 モデルの入力幅
    const MODEL_INPUT_HEIGHT = 640; // YOLOv8 モデルの入力高さ
    //検出閾値
    const SCORE_THRESHOLD = 0.5;  // 信頼度の閾値
    const IOU_THRESHOLD = 0.45;   // IoU (Intersection over Union) の閾値
    const CLASS_NAMES = [
        'clip',
    ];
    //色の設定
    const classColors = {};
    CLASS_NAMES.forEach((className) => {
        classColors[className] = '#' + Math.floor(Math.random()*16777215).toString(16).padStart(6, '0');
    });

    // DOM要素の取得と初期化
    function initDOMElements() {
        video = document.getElementById('input');
        canvas = document.getElementById('output');
        messageDiv = document.getElementById('message');
        messageText = document.getElementById('message-text');
        loadingIndicator = document.getElementById('loading');
        startButton = document.getElementById('start');
        stopButton = document.getElementById('stop');
        ctx = canvas.getContext('2d');
    }

    //イベントリスナーの設定
    function setupEventListeners() {
        window.addEventListener('resize', resizeCanvas);
        startButton.addEventListener('click', startDetection);
        stopButton.addEventListener('click', stopDetection);
    }
    // カメラの起動
    async function startCamera() {
        const constraints = {
            video: {
                facingMode: 'environment',
                width: { ideal: 2520 },
                height: { ideal: 1080 }
            }
        };
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        await video.play();
    }
    // YOLOv8 モデルの読み込み
    async function loadModel() {
        // 重要: GitHub Pagesでは相対パスでモデルを指定
        model = await tf.loadGraphModel('./best_web_model/model.json');  // best_web_model フォルダに配置
        return model;
    }

    //検出の前処理
    function preprocess(video) {
        let tensor = tf.browser.fromPixels(video)
        const resized = tensor.resizeBilinear([MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT])
        const batched = resized.expandDims(0)
        return batched.toFloat().div(tf.scalar(255))   // 0-255 -> 0-1 に正規化
    }

    // NMS (Non-Maximum Suppression)
    async function nonMaxSuppression(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold) {
      const boxes_ = tf.tensor2d(boxes);
      const scores_ = tf.tensor1d(scores);
      const nmsIndex = await tf.image.nonMaxSuppressionAsync(
        boxes_,
        scores_,
        maxOutputSize,
        iouThreshold,
        scoreThreshold
      );

      const selectedIndices = nmsIndex.dataSync();
      nmsIndex.dispose();
      boxes_.dispose();
      scores_.dispose();
      return selectedIndices;
    }

    // 検出結果を後処理して、描画可能な形にする関数
    async function postprocess(outputs, originalWidth, originalHeight) {

      const numPredictions = outputs.shape[1]; // 例: [1, 8400, 85] -> 8400
      const numClasses = outputs.shape[2] - 5;    // 例: [1, 8400, 85] -> 80 (クラス数)

      const data = await outputs.data(); // データを取得 (同期処理)

      const boxes = [];
      const scores = [];
      const classIds = [];

      // バウンディングボックス、スコア、クラスIDの抽出
      for (let i = 0; i < numPredictions; i++) {
        const offset = i * (numClasses + 5);
        const x = data[offset];
        const y = data[offset + 1];
        const width = data[offset + 2];
        const height = data[offset + 3];
        const confidence = data[offset + 4];

        if(confidence < SCORE_THRESHOLD) continue;

        const classScores = data.slice(offset + 5, offset + 5 + numClasses);
        let maxScore = -Infinity;
        let maxClassId = -1;

        for (let j = 0; j < numClasses; j++) {
          if (classScores[j] > maxScore) {
            maxScore = classScores[j];
            maxClassId = j;
          }
        }

        if (maxScore > SCORE_THRESHOLD) {
          boxes.push([(x - width / 2), (y - height / 2), (x + width / 2), (y + height / 2)]); //xywhからxyxy形式へ
          scores.push(maxScore);
          classIds.push(maxClassId);
        }
      }
        // Non-Maximum Suppression (NMS) を適用
        const nmsIndices = await nonMaxSuppression(
          boxes,
          scores,
          10, //maxOutputSize
          IOU_THRESHOLD,
          SCORE_THRESHOLD
        );

        const predictions = [];
        nmsIndices.forEach(index => {
          const box = boxes[index];
          const [x1, y1, x2, y2] = box;

          // 元の画像のサイズにスケールバック
          const scaleX = originalWidth / MODEL_INPUT_WIDTH;
          const scaleY = originalHeight / MODEL_INPUT_HEIGHT;

          predictions.push({
              bbox: {
                  x: x1 * scaleX,
                  y: y1 * scaleY,
                  width: (x2 - x1) * scaleX,
                  height: (y2 - y1) * scaleY
              },
              class: CLASS_NAMES[classIds[index]],
              score: scores[index]
          });
        });

        return predictions;
    }

    // 検出処理の開始
    async function startDetection() {
      try {
        updateMessage("モデルを読み込み中...");
        showLoading(true);
        startButton.disabled = true;

        if(!model){
          model = await loadModel();
        }

        updateMessage("カメラを起動中...");
        await startCamera();

        isRunning = true;
        stopButton.disabled = false;
        updateMessage("検出中", 2000);
        showLoading(false);
        detectFrame();

      } catch (error) {
          handleDetectionError(error);
      }
    }

    //検出処理の停止
    async function stopDetection() {
      isRunning = false;
      if (animationFrameId) {
        cancelAnimationFrame(animationFrameId);
      }

      if (video.srcObject) {
        video.srcObject.getTracks().forEach(track => track.stop());
        video.srcObject = null;
      }

      ctx.clearRect(0, 0, canvas.width, canvas.height);  //ビデオ停止時にキャンバスを消去する
      startButton.disabled = false;
      stopButton.disabled = true;
      updateMessage('停止しました');
    }
    // フレームごとの検出処理
        async function detectFrame() {
            if (!isRunning) return;

            const tensor = preprocess(video);
            const outputs = await model.executeAsync(tensor);

            // outputs の構造を確認 (コンソールに出力)
            console.log("outputs:", outputs);

            // outputs はオブジェクトなので、[0]でテンソル本体へアクセス
            const postprocessedPredictions = await postprocess(
              outputs[0], // outputs[0]を渡す
              video.videoWidth,
              video.videoHeight
            );

            tf.dispose(tensor);
            // outputs[0] がテンソル本体
            outputs[0].dispose();

            drawPredictions(postprocessedPredictions);
            animationFrameId = requestAnimationFrame(detectFrame);
        }

    // 検出結果の描画
    function drawPredictions(predictions) {
      ctx.clearRect(0, 0, canvas.width, canvas.height);  //描画前に毎回消去を行う
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

      predictions.forEach(prediction => {
        const {x, y, width, height} = prediction.bbox;
        const className = prediction.class;
        const color = classColors[className] || '#FFFFFF'; // クラスに対応する色を取得

        // Draw bounding box
        ctx.strokeStyle = color;
        ctx.lineWidth = 4;
        ctx.strokeRect(x, y, width, height);

        // Draw label
        ctx.fillStyle = color;
        ctx.font = '16px Arial';
        ctx.fillText(`${className} ${Math.round(prediction.score * 100)}%`, x, y - 5);
      });
    }

    // エラー処理
    function handleDetectionError(error) {
        console.error('Error:', error);
        let errorMessage = 'エラーが発生しました';
        if (error.name === 'NotAllowedError') {
            errorMessage = 'カメラへのアクセスが許可されていません';
        } else if (error.name === 'NotFoundError') {
            errorMessage = 'カメラが見つかりません';
        } else if (error.name === 'NotReadableError') { //追加：カメラが他のアプリで使用されている
            errorMessage = "カメラが他のアプリケーションによって使用されているため、アクセスできません。"
        }
        updateMessage(errorMessage);
        stopDetection(); // エラーが発生した場合、検出を停止
    }

    // キャンバスのリサイズ
    function resizeCanvas() {
        const container = document.querySelector('.container');
        canvas.width = container.clientWidth;
        canvas.height = container.clientHeight;
    }

    // ローディング表示の切り替え
    function showLoading(show) {
        loadingIndicator.style.display = show ? 'block' : 'none';
    }

    // メッセージの更新
    function updateMessage(text, timeout = 0) {
        messageText.textContent = text;
        if (timeout > 0) {
            setTimeout(() => {
                messageText.textContent = '';
            }, timeout);
        }
    }

    // グローバルエラーハンドラ
    window.onerror = function (msg, url, lineNo, columnNo, error) {
        console.error('Global error:', msg, url, lineNo, columnNo, error);
        updateMessage('エラーが発生しました: ' + msg);
        showLoading(false);
        startButton.disabled = false; // エラーが発生しても、開始ボタンは再度有効にする
        stopButton.disabled = true;  // 停止ボタンは無効のままにする
        return false;
    };

    // ページのロード完了時に実行
    window.addEventListener('load', async () => {
      initDOMElements();        //DOM要素の取得
      setupEventListeners();     //イベントリスナーの設定
      resizeCanvas();             //キャンバスのサイズを設定
      updateMessage('準備完了');    //準備完了メッセージを表示
    });
  </script>
</body>
</html>