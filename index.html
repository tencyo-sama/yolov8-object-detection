<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>YOLOv8 Object Detection</title>
  <!-- TensorFlow.js の読み込み (バージョンを指定) -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.2.0/dist/tf.min.js"></script>
  <style>
  body {
      margin: 0;
      padding: 0;
      width: 100vw;
      height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      background: #000;
    }
    .container {
      /* コンテナのサイズを調整 */
      width: 640px; /* モデルの入力サイズに合わせる */
      height: 480px; /* または、カメラの解像度に合わせて調整 */
      position: relative;
      overflow: hidden;
    }
    #output {
      width: 100%;
      height: 100%;
      /* object-fit: cover;  コンテナに合わせる場合はこれを使う */
    }
    /* ... (その他のスタイルは変更なし) ... */
        .button-container {
      position: fixed;
      bottom: 20px;
      width: 100%;
      display: flex;
      justify-content: center;
      gap: 20px;
      z-index: 1000;
    }
    button {
      padding: 15px 30px;
      font-size: 18px;
      border-radius: 25px;
      border: none;
      background: rgba(255, 255, 255, 0.9);
      box-shadow: 0 2px 5px rgba(0,0,0,0.2);
      cursor: pointer;
    }
    button:disabled {
      background: rgba(200, 200, 200, 0.9);
      cursor: not-allowed;
    }
    .message {
      position: fixed;
      bottom: 100px;
      left: 50%;
      transform: translateX(-50%);
      background-color: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 10px 20px;
      border-radius: 5px;
      font-size: 16px;
      z-index: 1000;
      text-align: center;
      max-width: 80%;
    }
    .loading-indicator {
      border: 4px solid #f3f3f3;
      border-radius: 50%;
      border-top: 4px solid #3498db;
      width: 20px;
      height: 20px;
      animation: spin 1s linear infinite;
      display: none;
      margin: 10px auto;
    }
    @keyframes spin {
      0% { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }
  </style>
</head>
<body>
  <div class="container">
    <canvas id="output"></canvas>
    <video id="input" style="display: none;" playsinline autoplay muted></video>
  </div>
  <div class="message" id="message">
    <div class="loading-indicator" id="loading"></div>
    <div id="message-text"></div>
  </div>
  <div class="button-container">
    <button id="start">開始</button>
    <button id="stop" disabled>停止</button>
  </div>

  <script>
    let model;
    let isRunning = false;
    let video, canvas, messageDiv, messageText, loadingIndicator, startButton, stopButton, ctx;
    let animationFrameId;

    // モデルの入力サイズ (YOLOv8 で学習させたサイズ)
    const MODEL_INPUT_WIDTH = 640;
    const MODEL_INPUT_HEIGHT = 640;

    // 検出閾値
    const SCORE_THRESHOLD = 0.5;  // 信頼度の閾値
    const IOU_THRESHOLD = 0.45;   // IoU (Intersection over Union) の閾値

    // クラス名 (今回は Clip のみ)
    const CLASS_NAMES = ['clip'];
        //色の設定
    const classColors = {};
    CLASS_NAMES.forEach((className) => {
        classColors[className] = '#' + Math.floor(Math.random()*16777215).toString(16).padStart(6, '0');
    });

    // DOM要素の取得と初期化
    function initDOMElements() {
        video = document.getElementById('input');
        canvas = document.getElementById('output');
        messageDiv = document.getElementById('message');
        messageText = document.getElementById('message-text');
        loadingIndicator = document.getElementById('loading');
        startButton = document.getElementById('start');
        stopButton = document.getElementById('stop');
        ctx = canvas.getContext('2d');
    }

    // イベントリスナーの設定
    function setupEventListeners() {
        window.addEventListener('resize', resizeCanvas);
        startButton.addEventListener('click', startDetection);
        stopButton.addEventListener('click', stopDetection);
    }

    // カメラの起動
    async function startCamera() {
      try {
        const constraints = {
          video: {
            facingMode: 'environment', // 背面カメラ
            width: { ideal: 640 },   // 希望する幅 (exact だと動かない場合がある)
            height: { ideal: 480 }   // 希望する高さ
          }
        };
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        await video.play();

        // カメラの解像度を確認
        console.log("Camera resolution:", video.videoWidth, video.videoHeight);

        // video 要素のサイズが確定したら、キャンバスをリサイズ
        video.addEventListener('loadedmetadata', () => {
          resizeCanvas();
        });

      } catch (error) {
        console.error("Error accessing camera:", error);
        handleDetectionError(error); // エラーハンドリング関数を呼び出す
      }
    }

    // モデルの読み込み
   async function loadModel() {
        model = await tf.loadGraphModel('./best_web_model/model.json');
        return model;
    }

    // 前処理 (リサイズ、正規化)
    function preprocess(video) {
      const tensor = tf.browser.fromPixels(video);
      const resized = tensor.resizeBilinear([MODEL_INPUT_HEIGHT, MODEL_INPUT_WIDTH]); // リサイズ
      const batched = resized.expandDims(0); // バッチ次元を追加
      return batched.toFloat().div(tf.scalar(255)); // 0-255 -> 0-1 に正規化
    }

    // 後処理 (NMS, 座標変換)
     async function postprocess(outputTensor, originalWidth, originalHeight) {
        const numPredictions = outputTensor.shape[1]; // 予測の数 (例: 8400)
        const numClasses = outputTensor.shape[2] - 5; // クラス数 (例: 80)  今回は1つしかないはず
        const data = await outputTensor.data(); // データを取得

        const boxes = [];
        const scores = [];
        const classIds = [];

      // 各予測について
        for (let i = 0; i < numPredictions; i++) {
          const offset = i * (numClasses + 5);

        // データの抽出
          let x = data[offset];        // バウンディングボックスの中心 x 座標
          let y = data[offset + 1];      // バウンディングボックスの中心 y 座標
          let width = data[offset + 2];  // バウンディングボックスの幅
          let height = data[offset + 3]; // バウンディングボックスの高さ
          const confidence = data[offset + 4]; // 信頼度

            // デバッグ: 生の値を出力
            console.log("Raw prediction data:", x, y, width, height, confidence);

            // 信頼度が低い場合はスキップ
            if (confidence < SCORE_THRESHOLD) continue;

            // クラススコアの処理 (今回はクラスが1つなので不要)
            /*
            const classScores = data.slice(offset + 5, offset + 5 + numClasses);
            let maxScore = -Infinity;
            let maxClassId = -1;
            for (let j = 0; j < numClasses; j++) {
                if (classScores[j] > maxScore) {
                    maxScore = classScores[j];
                    maxClassId = j;
                }
            }
            if (maxScore < SCORE_THRESHOLD) continue;
            */

            // xywh から xyxy 形式に変換 (モデルの出力形式に合わせて調整)
            let x1 = x - width / 2;
            let y1 = y - height / 2;
            let x2 = x + width / 2;
            let y2 = y + height / 2;

            // デバッグ: 変換後の座標を出力
            console.log("Transformed coordinates (before scale):", x1, y1, x2, y2);

            // スケールバック (モデルの出力が正規化されている場合)
            x1 *= originalWidth;
            y1 *= originalHeight;
            x2 *= originalWidth;
            y2 *= originalHeight;

            // デバッグ: スケールバック後の座標を出力
            console.log("Scaled coordinates:", x1, y1, x2, y2);


            boxes.push([x1, y1, x2, y2]); // xyxy 形式で保存
            scores.push(confidence); // 信頼度をそのまま使用 (クラス分類がないため)
            classIds.push(0); // クラスは常に 0 (Clip)
        }

        // NMS (Non-Maximum Suppression)
        const nmsIndices = await tf.image.nonMaxSuppressionAsync(
            tf.tensor2d(boxes),
            tf.tensor1d(scores),
            10, // maxOutputSize
            IOU_THRESHOLD, // iouThreshold
            SCORE_THRESHOLD // scoreThreshold
        );

        const predictions = [];
        // NMS で選択された予測のみを処理
        for (const i of nmsIndices.dataSync()) {
            const box = boxes[i];
            const score = scores[i];
            const classId = classIds[i];

            predictions.push({
                bbox: {
                    x: box[0],
                    y: box[1],
                    width: box[2] - box[0],
                    height: box[3] - box[1]
                },
                class: CLASS_NAMES[classId],
                score: score
            });
        }
        return predictions;
    }

    // 検出処理
    async function detectFrame() {
      if (!isRunning) return;

      const tensor = preprocess(video);
      try {
        const outputs = await model.executeAsync(tensor);

        // モデルの出力形式を確認 (最初の1回だけ)
        if (!model.outputShapes) {
          model.outputShapes = outputs.map(t => t.shape);
          console.log("Model output shapes:", model.outputShapes);
        }
        
        // 出力テンソルを取り出す (例: outputs[0] または outputs.output0 など)
        const outputTensor = outputs[0]; //

        const postprocessedPredictions = await postprocess(
          outputTensor, // outputTensor を渡す
          video.videoWidth,
          video.videoHeight
        );

        tf.dispose(tensor);
        tf.dispose(outputTensor); // outputTensor を解放

        drawPredictions(postprocessedPredictions);
      } catch (error) {
        console.error("Error in detectFrame:", error);
        updateMessage("推論中にエラーが発生しました");
        stopDetection();
      }

      animationFrameId = requestAnimationFrame(detectFrame);
    }

    // 描画処理
    function drawPredictions(predictions) {
      ctx.clearRect(0, 0, canvas.width, canvas.height); // キャンバスをクリア
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height); // カメラ映像を描画

      // バウンディングボックスとラベルを描画
      predictions.forEach(prediction => {
        const { x, y, width, height } = prediction.bbox;
        const className = prediction.class;
        const score = prediction.score;

        ctx.strokeStyle = '#00FF00'; // 緑色
        ctx.lineWidth = 2;
        ctx.strokeRect(x, y, width, height);

        ctx.fillStyle = '#00FF00';
        ctx.font = '16px Arial';
        ctx.fillText(`${className} ${Math.round(score * 100)}%`, x, y - 5);
      });
    }

    // エラーハンドリング
    function handleDetectionError(error) {
      console.error('Error:', error);
      let errorMessage = 'エラーが発生しました';
      if (error.name === 'NotAllowedError') {
        errorMessage = 'カメラへのアクセスが許可されていません';
      } else if (error.name === 'NotFoundError') {
        errorMessage = 'カメラが見つかりません';
      } else if (error.name === 'NotReadableError') {
        errorMessage = 'カメラが他のアプリケーションによって使用されているため、アクセスできません。';
      } else if (error.name === 'OverconstrainedError') {
        errorMessage = 'カメラの解像度設定が高すぎます。';
      }
      updateMessage(errorMessage);
      stopDetection(); // エラーが発生した場合、検出を停止
    }

    // キャンバスのリサイズ (video 要素のサイズに合わせる)
    function resizeCanvas() {
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
    }

    // ローディング表示の切り替え
    function showLoading(show) {
      loadingIndicator.style.display = show ? 'block' : 'none';
    }

    // メッセージの更新
    function updateMessage(text, timeout = 0) {
      messageText.textContent = text;
      if (timeout > 0) {
        setTimeout(() => {
          messageText.textContent = '';
        }, timeout);
      }
    }

    // グローバルエラーハンドラ
    window.onerror = function(msg, url, lineNo, columnNo, error) {
      console.error('Global error:', msg, url, lineNo, columnNo, error);
      updateMessage('エラーが発生しました: ' + msg);
      showLoading(false);
      startButton.disabled = false;
      stopButton.disabled = true;
      return false;
    };

      // 検出処理の開始
    async function startDetection() {
      try {
        updateMessage("モデルを読み込み中...");
        showLoading(true);
        startButton.disabled = true;

        if(!model){
          model = await loadModel();
        }

        updateMessage("カメラを起動中...");
        await startCamera();

        isRunning = true;
        stopButton.disabled = false;
        updateMessage("検出中", 2000);
        showLoading(false);
        detectFrame();

      } catch (error) {
          handleDetectionError(error);
      }
    }

    //検出処理の停止
    async function stopDetection() {
      isRunning = false;
      if (animationFrameId) {
        cancelAnimationFrame(animationFrameId);
      }

      if (video.srcObject) {
        video.srcObject.getTracks().forEach(track => track.stop());
        video.srcObject = null;
      }

      ctx.clearRect(0, 0, canvas.width, canvas.height);  //ビデオ停止時にキャンバスを消去する
      startButton.disabled = false;
      stopButton.disabled = true;
      updateMessage('停止しました');
    }

    // ページの読み込み完了時に実行
    window.addEventListener('load', async () => {
      initDOMElements();
      setupEventListeners();
      updateMessage('準備完了');
    });
  </script>
</body>
</html>